{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d9f811",
   "metadata": {},
   "source": [
    "# 2.Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dff89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "iris = sk.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e53600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the data matrix\n",
    "data = iris.data\n",
    "\n",
    "# Visualize the data matrix\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adf1bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier to predict the target values and report its performance using an appropriate evaluation metric.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14102eaa",
   "metadata": {},
   "source": [
    "### key parameters of random forest\n",
    "The random forest classifier is a popular ensemble learning algorithm that combines multiple decision trees to make predictions. It has several key parameters that can influence its performance. Let’s discuss some of these parameters and their impact:\n",
    "\n",
    "- Number of Trees:\n",
    "\n",
    "Increasing the number of trees typically improves the performance of the random forest. However, there is a trade-off between performance and computational cost. A larger number of trees may take longer to train and predict.\n",
    "\n",
    "- Max Features:\n",
    "\n",
    "This parameter determines the number of features randomly selected at each split. A smaller value can reduce the complexity of each tree and increase diversity among the trees. It can prevent overfitting, especially when dealing with high-dimensional data.\n",
    "Choosing max features equal to the total number of features is usually good for classification tasks, whereas for regression tasks, a smaller value, like the square root of the total number of features, can work well.\n",
    "\n",
    "- Max Depth:\n",
    "\n",
    "It sets the maximum depth allowed for each decision tree. A deeper tree can learn more complex relationships in the data, but it also increases the risk of overfitting. Setting a smaller max depth can help control overfitting.\n",
    "\n",
    "- Min Samples Split:\n",
    "\n",
    "This parameter defines the minimum number of samples required to split a node. A smaller value can make the model more sensitive to noise and potentially overfit, while a larger value can lead to underfitting by reducing the tree’s ability to divide the data.\n",
    "\n",
    "- Min Samples Leaf:\n",
    "\n",
    "It sets the minimum number of samples required to be at a leaf node. A smaller value allows the trees to be more specific to individual samples, potentially overfitting the training data. A larger value promotes generalization by resulting in more samples in each leaf.\n",
    "\n",
    "- Bootstrap Samples:\n",
    "\n",
    "Random forest uses bootstrapping by default, which means that each tree is trained on a random subset of the training data with replacement. Controlling the size of this subset can influence the diversity among the trees. A smaller value may reduce diversity, while a larger value may make the trees more similar, leading to less variance reduction.\n",
    "It’s important to note that the impact of these parameters can vary depending on the dataset and the specific problem at hand. To find the optimal values for these parameters, various techniques like cross-validation or grid search can be employed to tune them and evaluate the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae416c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
